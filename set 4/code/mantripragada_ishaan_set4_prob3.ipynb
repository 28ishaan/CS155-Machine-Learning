{"cells":[{"cell_type":"markdown","metadata":{"id":"GgBGICalwtPc"},"source":["# Problem 3"]},{"cell_type":"markdown","metadata":{"id":"5a4s16M5wtPe"},"source":["Use this notebook to write your code for problem 3."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uCkTD8rPwtPe","executionInfo":{"status":"ok","timestamp":1707369619139,"user_tz":480,"elapsed":221,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"0OqakB12wtPf"},"source":["## 3D - Convolutional network"]},{"cell_type":"markdown","metadata":{"id":"OLmGpiRLwtPf"},"source":["As in problem 2, we have conveniently provided for your use code that loads and preprocesses the MNIST data."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYNM6gTywtPg","executionInfo":{"status":"ok","timestamp":1707369631500,"user_tz":480,"elapsed":12362,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"5e0bf331-02f2-458d-aa0a-0c2b6c8f481a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 108758076.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 44453465.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 47010742.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 12725804.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# load MNIST data into PyTorch format\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# set batch size\n","batch_size = 32\n","\n","# load training data downloaded into data/ folder\n","mnist_training_data = torchvision.datasets.MNIST('data/', train=True, download=True,\n","                                                transform=transforms.ToTensor())\n","# transforms.ToTensor() converts batch of images to 4-D tensor and normalizes 0-255 to 0-1.0\n","training_data_loader = torch.utils.data.DataLoader(mnist_training_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=True)\n","\n","# load test data\n","mnist_test_data = torchvision.datasets.MNIST('data/', train=False, download=True,\n","                                                transform=transforms.ToTensor())\n","test_data_loader = torch.utils.data.DataLoader(mnist_test_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NRkieeIwtPg","executionInfo":{"status":"ok","timestamp":1707369631500,"user_tz":480,"elapsed":8,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"93965c94-5877-4b51-d1aa-cfcff65a62b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1875 training batches\n","60000 training samples\n","313 validation batches\n"]}],"source":["# look at the number of batches per epoch for training and validation\n","print(f'{len(training_data_loader)} training batches')\n","print(f'{len(training_data_loader) * batch_size} training samples')\n","print(f'{len(test_data_loader)} validation batches')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDqC9EFcwtPh"},"outputs":[],"source":["# sample model\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(1, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Conv2d(8, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYr9Gd9QwtPh","executionInfo":{"status":"ok","timestamp":1706859657331,"user_tz":480,"elapsed":191,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"fd1677ba-8cba-46b9-9a2a-237f2e584ae7"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1, 3, 3])\n","torch.Size([8])\n","torch.Size([8, 8, 3, 3])\n","torch.Size([8])\n","torch.Size([64, 200])\n","torch.Size([64])\n","torch.Size([10, 64])\n","torch.Size([10])\n"]}],"source":["# why don't we take a look at the shape of the weights for each layer\n","for p in model.parameters():\n","    print(p.data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbAv-PPKwtPh","executionInfo":{"status":"ok","timestamp":1706859660871,"user_tz":480,"elapsed":177,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"c28e63e1-6ed7-49a9-f366-f9d568d76a83"},"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 14178\n"]}],"source":["# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqo3Xxg8wtPh"},"outputs":[],"source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-MUI7yYwtPh","executionInfo":{"status":"ok","timestamp":1706859898026,"user_tz":480,"elapsed":231009,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"7e8f760a-9fa4-4b0f-ed0a-84895a6bd007"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.7707, acc: 0.7498, val loss: 0.1970, val acc: 0.9395\n","Epoch 2/10:...........\n","\tloss: 0.4323, acc: 0.8638, val loss: 0.1714, val acc: 0.9510\n","Epoch 3/10:...........\n","\tloss: 0.4024, acc: 0.8753, val loss: 0.1528, val acc: 0.9566\n","Epoch 4/10:...........\n","\tloss: 0.3935, acc: 0.8793, val loss: 0.1412, val acc: 0.9591\n","Epoch 5/10:...........\n","\tloss: 0.3900, acc: 0.8809, val loss: 0.1615, val acc: 0.9581\n","Epoch 6/10:...........\n","\tloss: 0.3850, acc: 0.8804, val loss: 0.1517, val acc: 0.9577\n","Epoch 7/10:...........\n","\tloss: 0.3826, acc: 0.8806, val loss: 0.1188, val acc: 0.9652\n","Epoch 8/10:...........\n","\tloss: 0.3817, acc: 0.8829, val loss: 0.1522, val acc: 0.9602\n","Epoch 9/10:...........\n","\tloss: 0.3808, acc: 0.8845, val loss: 0.1270, val acc: 0.9689\n","Epoch 10/10:...........\n","\tloss: 0.3671, acc: 0.8895, val loss: 0.1549, val acc: 0.9615\n"]}],"source":["# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"82Y0Mt-swtPi"},"source":["Above, we output the training loss/accuracy as well as the validation loss and accuracy. Not bad! Let's see if you can do better."]},{"cell_type":"markdown","source":["# Problem G"],"metadata":{"id":"9WHQkgkzykVi"}},{"cell_type":"markdown","source":["### Dropout probabilities"],"metadata":{"id":"RPQAxp1O0XM6"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","dropout_probabilities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","\n","for i in range(11):\n","  print(f'Dropout probabilty: {dropout_probabilities[i]}')\n","\n","  # define model with specific dropout probability\n","  model = nn.Sequential(\n","      nn.Conv2d(1, 8, kernel_size=(3,3)),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2),\n","      nn.Dropout(p=dropout_probabilities[i]),\n","\n","      nn.Conv2d(8, 8, kernel_size=(3,3)),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2),\n","      nn.Dropout(p=dropout_probabilities[i]),\n","\n","      nn.Flatten(),\n","      nn.Linear(25*8, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, 10)\n","      # PyTorch implementation of cross-entropy loss includes softmax layer\n","  )\n","\n","  # For a multi-class classification problem\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.RMSprop(model.parameters())\n","\n","  # Train the model for 10 epochs, iterating on the data in batches\n","  n_epochs = 1\n","\n","  # store metrics\n","  training_accuracy_history = np.zeros([n_epochs, 1])\n","  training_loss_history = np.zeros([n_epochs, 1])\n","  validation_accuracy_history = np.zeros([n_epochs, 1])\n","  validation_loss_history = np.zeros([n_epochs, 1])\n","\n","  for epoch in range(1):\n","      print(f'Epoch {epoch+1}:', end='')\n","      train_total = 0\n","      train_correct = 0\n","      # train\n","      model.train()\n","      for i, data in enumerate(training_data_loader):\n","          images, labels = data\n","          optimizer.zero_grad()\n","          # forward pass\n","          output = model(images)\n","          # calculate categorical cross entropy loss\n","          loss = criterion(output, labels)\n","          # backward pass\n","          loss.backward()\n","          optimizer.step()\n","\n","          # track training accuracy\n","          _, predicted = torch.max(output.data, 1)\n","          train_total += labels.size(0)\n","          train_correct += (predicted == labels).sum().item()\n","          # track training loss\n","          training_loss_history[epoch] += loss.item()\n","          # progress update after 180 batches (~1/10 epoch for batch size 32)\n","          if i % 180 == 0: print('.',end='')\n","      training_loss_history[epoch] /= len(training_data_loader)\n","      training_accuracy_history[epoch] = train_correct / train_total\n","      print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","      # validate\n","      test_total = 0\n","      test_correct = 0\n","      with torch.no_grad():\n","          model.eval()\n","          for i, data in enumerate(test_data_loader):\n","              images, labels = data\n","              # forward pass\n","              output = model(images)\n","              # find accuracy\n","              _, predicted = torch.max(output.data, 1)\n","              test_total += labels.size(0)\n","              test_correct += (predicted == labels).sum().item()\n","              # find loss\n","              loss = criterion(output, labels)\n","              validation_loss_history[epoch] += loss.item()\n","          validation_loss_history[epoch] /= len(test_data_loader)\n","          validation_accuracy_history[epoch] = test_correct / test_total\n","      print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-l-PuPFyypp5","executionInfo":{"status":"ok","timestamp":1706861026283,"user_tz":480,"elapsed":245836,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"52d73b6e-7490-4519-e75d-6148052d6b9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dropout probabilty: 0\n","Epoch 1:...........\n","\tloss: 0.2960, acc: 0.9077, val loss: 0.1808, val acc: 0.9444\n","Dropout probabilty: 0.1\n","Epoch 1:...........\n","\tloss: 0.3631, acc: 0.8872, val loss: 0.1230, val acc: 0.9618\n","Dropout probabilty: 0.2\n","Epoch 1:...........\n","\tloss: 0.3841, acc: 0.8849, val loss: 0.1038, val acc: 0.9686\n","Dropout probabilty: 0.3\n","Epoch 1:...........\n","\tloss: 0.4281, acc: 0.8760, val loss: 0.1305, val acc: 0.9602\n","Dropout probabilty: 0.4\n","Epoch 1:...........\n","\tloss: 1.2842, acc: 0.5237, val loss: 0.1925, val acc: 0.9497\n","Dropout probabilty: 0.5\n","Epoch 1:...........\n","\tloss: 0.7564, acc: 0.7462, val loss: 0.2887, val acc: 0.9313\n","Dropout probabilty: 0.6\n","Epoch 1:...........\n","\tloss: 0.9391, acc: 0.6875, val loss: 0.3984, val acc: 0.8884\n","Dropout probabilty: 0.7\n","Epoch 1:...........\n","\tloss: 1.3519, acc: 0.5323, val loss: 0.6488, val acc: 0.8746\n","Dropout probabilty: 0.8\n","Epoch 1:...........\n","\tloss: 1.3765, acc: 0.5319, val loss: 0.9178, val acc: 0.7946\n","Dropout probabilty: 0.9\n","Epoch 1:...........\n","\tloss: 2.1450, acc: 0.2282, val loss: 2.3389, val acc: 0.1137\n","Dropout probabilty: 1\n","Epoch 1:...........\n","\tloss: 2.3027, acc: 0.1096, val loss: 2.2975, val acc: 0.1032\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# define model with specific dropout probability\n","model = nn.Sequential(\n","    nn.Conv2d(1, 16, kernel_size=(3,3)),\n","    nn.BatchNorm2d(16),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Conv2d(8, 8, kernel_size=(3,3)),\n","    nn.BatchNorm2d(8),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")"],"metadata":{"id":"JadWMAoT22Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMOMIDHd226u","executionInfo":{"status":"ok","timestamp":1706862754614,"user_tz":480,"elapsed":146,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"d447a3b2-0805-4bba-d262-e69a77aba0be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 15466\n"]}]},{"cell_type":"code","source":["# For a multi-class classification problem\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())\n","\n","# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(10):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYhVrZ7W2Nif","executionInfo":{"status":"ok","timestamp":1706863174583,"user_tz":480,"elapsed":418710,"user":{"displayName":"Ishaan Mantripragada","userId":"15706881133196367759"}},"outputId":"343c1c65-0d7a-4a12-f577-523cc3f5af40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.3093, acc: 0.9100, val loss: 0.0895, val acc: 0.9732\n","Epoch 2/10:...........\n","\tloss: 0.1232, acc: 0.9629, val loss: 0.0885, val acc: 0.9692\n","Epoch 3/10:...........\n","\tloss: 0.1029, acc: 0.9683, val loss: 0.0917, val acc: 0.9705\n","Epoch 4/10:...........\n","\tloss: 0.0945, acc: 0.9711, val loss: 0.0622, val acc: 0.9811\n","Epoch 5/10:...........\n","\tloss: 0.0891, acc: 0.9741, val loss: 0.0571, val acc: 0.9822\n","Epoch 6/10:...........\n","\tloss: 0.0826, acc: 0.9751, val loss: 0.0496, val acc: 0.9849\n","Epoch 7/10:...........\n","\tloss: 0.0805, acc: 0.9762, val loss: 0.0551, val acc: 0.9833\n","Epoch 8/10:...........\n","\tloss: 0.0769, acc: 0.9771, val loss: 0.0554, val acc: 0.9830\n","Epoch 9/10:...........\n","\tloss: 0.0776, acc: 0.9771, val loss: 0.0513, val acc: 0.9853\n","Epoch 10/10:...........\n","\tloss: 0.0749, acc: 0.9773, val loss: 0.0505, val acc: 0.9850\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}